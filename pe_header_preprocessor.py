# ----------------------------------------------------------------------------
# Imports 
# ----------------------------------------------------------------------------

import pefile
import numpy as np
import glob
import logging

from utils import *
from common import *






# ----------------------------------------------------------------------------
# Classes 
# ----------------------------------------------------------------------------

class PeHeaderPreprocessor():

    def __init__(self, features_count):
        self._features_count = features_count

    def extract_api_functions_vector(self, filepath):
        try:
            pe = pefile.PE(filepath)

            api_functions_vector = []

            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                dll_entry_functions = [func.name.decode('utf-8') for func in entry.imports if func.name]
                api_functions_vector += (dll_entry_functions)

            #api_functions_vector = np.array(api_functions_vector)
            #np.savetxt('api_functions_vector_example.csv' , api_functions_vector.reshape(1, api_functions_vector.shape[0]) , fmt='%s', delimiter=',')
            #print(api_functions_vector.shape)

            return api_functions_vector

        except Exception as ex:
            logging.getLogger().error(str(ex))
            return []

            

    def extract_most_common_api_functions_used(self, in_directory_path, out_filepath):

        all_functions_dict = {}

        for filepath in glob.iglob(f'{in_directory_path}/*.exe'):

            print('Processing: ' + filepath)
            api_functions_vector = self.extract_api_functions_vector(filepath)

            for function_name in api_functions_vector:

                if not function_name in all_functions_dict:
                    all_functions_dict[function_name] = 1

                else:
                    all_functions_dict[function_name] = all_functions_dict[function_name] + 1


        # Now export that dictionary to csv file
        api_functions_counts_arr = [(func, count) for func, count in all_functions_dict.items()]
        api_functions_counts_arr = np.array(api_functions_counts_arr)

        CSV_FILE_HEADER = 'FUNCTION, COUNT'
        np.savetxt(out_filepath, api_functions_counts_arr , fmt='%s', delimiter=',', header=CSV_FILE_HEADER)



    def _create_partial_dataset(self, most_used_api_functions,  exe_files_input_dir):
       
        dataset_list = []

        for filepath in glob.iglob(f'{exe_files_input_dir}/*.exe'):

            print('Processing: ' + filepath)
            file_api_functions_vector = self.extract_api_functions_vector(filepath)
            if not file_api_functions_vector:
                continue

            feature_vector = [1 if func_name in file_api_functions_vector else 0 for func_name in most_used_api_functions]
            dataset_list.append(feature_vector)

        return np.array(dataset_list)
    

    def create_dataset(self, most_used_functions_file, benign_files_input_dir, malicious_files_input_dir, output_dataset_file):

        most_used_api_functions_mat = np.genfromtxt(most_used_functions_file, skip_header=True, dtype=str, delimiter=',')

        # Sort that array from most used functions first. 
        most_used_api_functions_mat = sorted(list(most_used_api_functions_mat), reverse=True, key=lambda pair : int(pair[1]))

        # Extract most used api functions, in count of features count. 
        most_used_api_functions_array = np.array(most_used_api_functions_mat)[0:self._features_count ,0]

        benign_partial_dataset = self._create_partial_dataset(most_used_api_functions_array, benign_files_input_dir)
        malicious_partial_dataset = self._create_partial_dataset(most_used_api_functions_array, malicious_files_input_dir)

        # push to each matrix 1 coulumn of label 
        benign_dataset = np.column_stack((benign_partial_dataset, np.ones(benign_partial_dataset.shape[0]) * LABEL_BENIGN))
        malicious_dataset = np.column_stack((malicious_partial_dataset, np.ones(malicious_partial_dataset.shape[0]) * LABEL_MALICIOUS))

        final_dataset = np.concatenate((benign_dataset, malicious_dataset))
        np.savetxt(output_dataset_file, final_dataset , fmt='%s', delimiter=',')




    def debug__analyze_file(self, filepath):
        pe = pefile.PE(filepath)
        print(f"[*] Listing imported DLLs of file:   {filepath} ")

        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            print('----------------------------------------------------------------------------')
            print('\t' + entry.dll.decode('utf-8'))
            print('\n')

            for func in entry.imports:
                if func.name:
                    print(func.name.decode('utf-8'))
                #print("\t%s at 0x%08x" % (func.name.decode('utf-8'), func.address))
                


