# ----------------------------------------------------------------------------
# Imports 
# ----------------------------------------------------------------------------

import numpy as np
from nltk import ngrams
import torch
import glob
import os
import logging

import pefile
import capstone

from utils import *
from common import *

# ----------------------------------------------------------------------------
# Globals and constants
# ----------------------------------------------------------------------------

OPCODE_OFFSET_IN_INSTRUCTION = 2

NGRAM_N = 2



# ----------------------------------------------------------------------------
# Classes 
# ----------------------------------------------------------------------------


class NgramPreprocessor():

	def __init__(self, features_count):
		self.features_count = features_count

	def export_to_csv(self, outfile_path, sequences_mat, sequence_freqs):
		# concatanete frequencies array to be the last column of the sequenes matrix
		sequence_freqs = sequence_freqs.reshape(len(sequence_freqs), 1)
		result_matrix = np.concatenate((sequences_mat, sequence_freqs), axis=1)

		CSV_FILE_HEADER = 'OPCODE1, OPCODE2, FREQUENCY'
		np.savetxt(outfile_path, result_matrix , fmt='%s', delimiter=',', header=CSV_FILE_HEADER)


	#@timeit
	def calc_ngram_pure_python(self,twogram_array):
		hist = [((hex(seq[0]),hex(seq[1])), twogram.count(seq)) for seq in unique_sequences[:HISTOGRAM_SIZE]]
		return hist

	#@timeit
	def calc_ngram_numpy(self,twogram_array):
		unique_sequences, sequence_freqs = np.unique(twogram_array, axis=0, return_counts=True)
		return (unique_sequences, sequence_freqs)


	#@timeit
	def calc_ngram_gpu_accelerated(self, twogram_array):
		device_gpu = torch.device("cuda:0")

		# convert array to tensor object and copy to GPU memory 
		twogram_tensor = torch.from_numpy(twogram_array)
		twogram_tensor = twogram_tensor.to(device_gpu)

		unique_sequences, sequence_freqs = torch.unique(twogram_tensor,  return_counts=True, dim=0)

		# Move tensor back to CPU memory, as numpy arrays
		unique_sequences = unique_sequences.detach().cpu().numpy()
		sequence_freqs = sequence_freqs.detach().cpu().numpy()

		return (unique_sequences, sequence_freqs)


	def cleanup_gpu_reasources(self):
		torch.cuda.empty_cache()


	def extract_opcodes_list(self, file_path, is_pe):

		if is_pe:
			pe = pefile.PE(file_path)

			# Find text (code) section and extract it
			for section in pe.sections:
				if 'text' in str(section.Name):
					code_section = section
					break

			raw_code = code_section.get_data()
			code_section_size = code_section.SizeOfRawData

			# Determine pe file architecture
			if pe.FILE_HEADER.Machine == 0x014c:
				pe_architecutre = capstone.CS_MODE_32

			if pe.FILE_HEADER.Machine == 0x8664:
				pe_architecutre = capstone.CS_MODE_64

		else:
			input_file = open(file_path, 'rb')
			pe_architecutre = capstone.CS_MODE_32
			raw_code = input_file.read()

			raw_code = np.trim_zeros(raw_code[50:])
			code_section_size = len(raw_code)

			input_file.close()


		# initialize disassembler to of the correct architecture 
		disassembler = capstone.Cs(capstone.CS_ARCH_X86, pe_architecutre)
		opcodes_list = []

		"""
		Go over all text section where binary code should be
		Disassemble every chunck and add the opcodes found to the opcodes list
		"""
		disassembler.skipdata = True
		disassembly_generator = disassembler.disasm_lite(raw_code, 0)

		for instruction in disassembly_generator:
			opcodes_list.append(instruction[OPCODE_OFFSET_IN_INSTRUCTION])

		opcodes_data_ratio = len(opcodes_list) / len(raw_code)
		print(f"----------> OpcodesCount/FileSize = : {opcodes_data_ratio}")




		return opcodes_list


	@timeit
	def extract_ngram_of_file(self, in_file_path, out_file_path, is_pe=True):

		try:
			opcodes_list = self.extract_opcodes_list(in_file_path, is_pe)
			print(f"Opcodes Count: {len(opcodes_list)}")

			#print (opcodes_list[:50])
			#print("\n")
			#print (opcodes_list[-50:])

		    # Now extrat twogram vector out of the binary file's opcodes list
			twogram_iter = ngrams(opcodes_list, NGRAM_N)
			twogram_array = np.array(list(twogram_iter))
			unique_sequences, sequence_freqs = self.calc_ngram_numpy(twogram_array)

			self.export_to_csv(out_file_path, unique_sequences, sequence_freqs)

		except Exception as ex:
			try:
				msg = f'==========> ERROR EXTRACTING NGRAM of: {in_file_path} \t|' + str(ex)
				logging.getLogger().error(msg)
			except:
				pass



	def process_all_files_in_dir(self, in_directory_path, out_directory_path, extention="exe"):

		for filepath in glob.iglob(f'{in_directory_path}/*.{extention}'):
			basename = os.path.basename(filepath)
			
			print('----------------------------------------------------------------------------')
			print('Processing:   ' + filepath)
			self.extract_ngram_of_file(filepath, f'{out_directory_path}/{basename}-ngram.csv', extention=="exe")
			print('----------------------------------------------------------------------------')

			

	def extract_most_common_sequences(self, in_directory_path, most_common_seqs_filepath):

		all_tfs_dict = {}

		for filepath in glob.iglob(f'{in_directory_path}/*.csv'):
			ngram_mat = np.genfromtxt(filepath ,skip_header=True, dtype=str, delimiter=',')
			term_frequecies = ngram_mat[:,NGRAM_N].astype(np.int32)
			opcodes_mat = ngram_mat[:,0:NGRAM_N]

			print(f'Processing: {filepath} ')

			for index, opcodes_sequence in enumerate(opcodes_mat):
				opcodes_sequence = tuple(opcodes_sequence.tolist())

				if not opcodes_sequence in all_tfs_dict:
					all_tfs_dict[opcodes_sequence] = term_frequecies[index]

				else:
					all_tfs_dict[opcodes_sequence] = all_tfs_dict[opcodes_sequence] + term_frequecies[index]


		# Now create numpy array from that dictionary and export it to dataset file
		sequenes_list = list(all_tfs_dict.keys())

		sequences_mat = np.array(sequenes_list)
		sequence_freqs = np.array(list(all_tfs_dict.values()))

		sequence_freqs = sequence_freqs.reshape(len(sequence_freqs), 1)
		result_matrix = np.concatenate((sequences_mat, sequence_freqs), axis=1)

		CSV_FILE_HEADER = 'OPCODE1, OPCODE2, COMBINED FREQUENCIES'
		np.savetxt(most_common_seqs_filepath, result_matrix , fmt='%s', delimiter=',', header=CSV_FILE_HEADER)


	def _create_partial_dataset(self, most_common_ngrams_file, ngram_files_input_dir, temp_filename):
		ngram_mat = np.genfromtxt(most_common_ngrams_file ,skip_header=True, dtype=str, delimiter=',')
		sorted_gram_mat = sorted(list(ngram_mat), reverse=True, key=lambda seq : int(seq[NGRAM_N]))

		most_common_ngram_features_mat = np.array(sorted_gram_mat)[:self.features_count]
		#common_sequence_freqs = most_common_ngram_features_mat[:,NGRAM_N].astype(np.int32)
		most_common_sequences_arr = most_common_ngram_features_mat[:,0:NGRAM_N]
		#print(most_common_sequences_arr)

		# For file in ngram_files_input_dir - create line in dataset file
		# For each sequence in most_common_ngram_features
		# if sequence not exsist in file_ngram ---> put 0 in vector cell
		# otherwise - put the sequen frequency which in the file (count/features count)

		dataset_list = []

		for filepath in glob.iglob(f'{ngram_files_input_dir}/*.csv'):
			print('Processing: ' + filepath)

			file_ngram_mat = np.genfromtxt(filepath ,skip_header=True, dtype=str, delimiter=',')
			ngram_total_features_count = sum(file_ngram_mat[:,NGRAM_N].astype(np.int32))

			# Build a dictionary of each of the file's sequences and it's term-frequency 
			file_ngrams_dict = {tuple(sequence[:NGRAM_N]) : float(sequence[NGRAM_N]) for sequence in file_ngram_mat}

			# Finally build a feature vector of the spesific file in the dataset 
			file_feature_vector = [file_ngrams_dict[tuple(sequence)]/ngram_total_features_count if tuple(sequence) in file_ngrams_dict else 0.0 for sequence in most_common_sequences_arr]
			dataset_list.append(file_feature_vector)


		# Convert dataset to numpy matrix and export dataset to csv file 
		dataset = np.array(dataset_list)
		np.savetxt(temp_filename, dataset , fmt='%s', delimiter=',')

		return dataset

	def build_dataset(self, most_common_ngrams_file, ngram_benign_files_input_dir, ngram_malicious_files_input_dir, output_dataset_file):

		benign_files_features_matrix = self._create_partial_dataset(most_common_ngrams_file, ngram_benign_files_input_dir, 'benign.csv')
		malicious_files_features_matrix = self._create_partial_dataset(most_common_ngrams_file, ngram_malicious_files_input_dir, 'malicious.csv')

		# push to each matrix 1 coulumn of label 
		benign_dataset = np.column_stack((benign_files_features_matrix, np.ones(benign_files_features_matrix.shape[0]) * LABEL_BENIGN))
		malicous_dataset = np.column_stack((malicious_files_features_matrix, np.ones(malicious_files_features_matrix.shape[0]) * LABEL_MALICIOUS))

		final_dataset = np.concatenate((benign_dataset,malicous_dataset))
		np.savetxt(output_dataset_file, final_dataset , fmt='%s', delimiter=',')



	def __del__(self):
		self.cleanup_gpu_reasources()