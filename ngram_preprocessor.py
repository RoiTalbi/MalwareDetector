# ----------------------------------------------------------------------------
# Imports 
# ----------------------------------------------------------------------------

import numpy as np
from nltk import ngrams
import torch
import glob
import os
import logging

import pefile
import capstone

from utils import *


# ----------------------------------------------------------------------------
# Globals and constants
# ----------------------------------------------------------------------------

OPCODE_OFFSET = 2

NGRAM_N = 2

FEATURES_COUNT = 1000

# ----------------------------------------------------------------------------
# Classes 
# ----------------------------------------------------------------------------


class NgramPreprocessor():

	def __init__(self):
		pass

	def export_to_csv(self, outfile_path, sequences_mat, sequence_freqs):
		# concatanete frequencies array to be the last column of the sequenes matrix
		sequence_freqs = sequence_freqs.reshape(len(sequence_freqs), 1)
		result_matrix = np.concatenate((sequences_mat, sequence_freqs), axis=1)

		CSV_FILE_HEADER = 'OPCODE1, OPCODE2, FREQUENCY'
		np.savetxt(outfile_path, result_matrix , fmt='%s', delimiter=',', header=CSV_FILE_HEADER)


	#@timeit
	def calc_ngram_pure_python(self,twogram_array):
		hist = [((hex(seq[0]),hex(seq[1])), twogram.count(seq)) for seq in unique_sequences[:HISTOGRAM_SIZE]]
		return hist

	#@timeit
	def calc_ngram_numpy(self,twogram_array):
		unique_sequences, sequence_freqs = np.unique(twogram_array, axis=0, return_counts=True)
		return (unique_sequences, sequence_freqs)


	#@timeit
	def calc_ngram_gpu_accelerated(self, twogram_array):
		device_gpu = torch.device("cuda:0")

		# convert array to tensor object and copy to GPU memory 
		twogram_tensor = torch.from_numpy(twogram_array)
		twogram_tensor = twogram_tensor.to(device_gpu)

		unique_sequences, sequence_freqs = torch.unique(twogram_tensor,  return_counts=True, dim=0)

		# Move tensor back to CPU memory, as numpy arrays
		unique_sequences = unique_sequences.detach().cpu().numpy()
		sequence_freqs = sequence_freqs.detach().cpu().numpy()

		return (unique_sequences, sequence_freqs)


	def cleanup_gpu_reasources(self):
		torch.cuda.empty_cache()


	def extract_opcodes_list(self, file_path, is_pe):

		if is_pe:
			pe = pefile.PE(file_path)

			# Find text (code) section and extract it
			for section in pe.sections:
				if 'text' in str(section.Name):
					code_section = section
					break

			raw_code = code_section.get_data()
			code_section_size = code_section.SizeOfRawData

			# Determine pe file architecture
			if pe.FILE_HEADER.Machine == 0x014c:
				pe_architecutre = capstone.CS_MODE_32

			if pe.FILE_HEADER.Machine == 0x8664:
				pe_architecutre = capstone.CS_MODE_64

		else:
			input_file = open(file_path, 'rb')
			pe_architecutre = capstone.CS_MODE_32
			raw_code = input_file.read()

			raw_code = np.trim_zeros(raw_code[50:])
			code_section_size = len(raw_code)

			input_file.close()


		# initialize disassembler to of the correct architecture 
		disassembler = capstone.Cs(capstone.CS_ARCH_X86, pe_architecutre)
		opcodes_list = []

		"""
		Go over all text section where binary code should be
		Disassemble every chunck and add the opcodes found to the opcodes list
		"""
		disassembler.skipdata = True
		disassembly_generator = disassembler.disasm_lite(raw_code, 0)

		for instruction in disassembly_generator:
			opcodes_list.append(instruction[OPCODE_OFFSET])

		opcodes_data_ratio = len(opcodes_list) / len(raw_code)
		print(f"----------> OpcodesCount/FileSize = : {opcodes_data_ratio}")




		return opcodes_list


	@timeit
	def extract_ngram_of_file(self, in_file_path, out_file_path, is_pe=True):

		try:
			opcodes_list = self.extract_opcodes_list(in_file_path, is_pe)
			print(f"Opcodes Count: {len(opcodes_list)}")

			#print (opcodes_list[:50])
			#print("\n")
			#print (opcodes_list[-50:])

		    # Now extrat twogram vector out of the binary file's opcodes list
			twogram_iter = ngrams(opcodes_list, NGRAM_N)
			twogram_array = np.array(list(twogram_iter))
			unique_sequences, sequence_freqs = self.calc_ngram_numpy(twogram_array)

			self.export_to_csv(out_file_path, unique_sequences, sequence_freqs)

		except Exception as ex:
			try:
				msg = f'==========> ERROR EXTRACTING NGRAM of: {in_file_path} \t|' + str(ex)
				logging.getLogger().error(msg)
			except:
				pass



	def process_all_files_in_dir(self, in_directory_path, out_directory_path, extention="exe"):

		for filepath in glob.iglob(f'{in_directory_path}/*.{extention}'):
			basename = os.path.basename(filepath)
			
			print('----------------------------------------------------------------------------')
			print('Processing:   ' + filepath)
			self.extract_ngram_of_file(filepath, f'{out_directory_path}/{basename}-ngram.csv', extention=="exe")
			print('----------------------------------------------------------------------------')

			

	def extract_most_common_sequences(self, in_directory_path, most_common_seqs_filepath):

		all_tfs_dict = {}

		for filepath in glob.iglob(f'{in_directory_path}/*.csv'):
			ngram_mat = np.genfromtxt(filepath ,skip_header=True, dtype=str, delimiter=',')
			term_frequecies = ngram_mat[:,NGRAM_N].astype(np.int32)
			opcodes_mat = ngram_mat[:,0:NGRAM_N]

			print(f'Processing: {filepath} ')

			for index, opcodes_sequence in enumerate(opcodes_mat):
				opcodes_sequence = tuple(opcodes_sequence.tolist())

				if not opcodes_sequence in all_tfs_dict:
					all_tfs_dict[opcodes_sequence] = term_frequecies[index]

				else:
					all_tfs_dict[opcodes_sequence] = all_tfs_dict[opcodes_sequence] + term_frequecies[index]


		# Now create numpy array from that dictionary and export it to dataset file
		sequenes_list = list(all_tfs_dict.keys())

		sequences_mat = np.array(sequenes_list)
		sequence_freqs = np.array(list(all_tfs_dict.values()))

		sequence_freqs = sequence_freqs.reshape(len(sequence_freqs), 1)
		result_matrix = np.concatenate((sequences_mat, sequence_freqs), axis=1)

		CSV_FILE_HEADER = 'OPCODE1, OPCODE2, COMBINED FREQUENCIES'
		np.savetxt(most_common_seqs_filepath, result_matrix , fmt='%s', delimiter=',', header=CSV_FILE_HEADER)



	def build_dataset(most_common_seqs_filepath):
		ngram_mat = np.genfromtxt(most_common_seqs_filepath ,skip_header=True, dtype=str, delimiter=',')
		sorted_gram_mat = sorted(list(ngram_mat), reverse=True, key=lambda seq : int(seq[NGRAM_N]))

		most_common_ngram_features = np.array(sorted_gram_mat)[:FEATURES_COUNT]
		common_sequence_freqs = ngram_mat[:,NGRAM_N].astype(np.int32)
		common_sequences = most_common_ngram_features[:,0:NGRAM_N]





	def __del__(self):
		self.cleanup_gpu_reasources()