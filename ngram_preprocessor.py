# ----------------------------------------------------------------------------
# Imports 
# ----------------------------------------------------------------------------

import numpy as np
from nltk import ngrams
import glob
import os
import logging

import pefile
import capstone

from preprocessor import Preprocessor
from utils import *
from common import *
from md_exception import *

# ----------------------------------------------------------------------------
# Globals and constants
# ----------------------------------------------------------------------------

OPCODE_OFFSET_IN_INSTRUCTION = 2

NGRAM_N = 2

TEMP_OUTPUT_FILENAME_MOST_COMMON_NGRAMS = 'NGRAM_PROCESSOR_most_common_ngrams.csv'



# ----------------------------------------------------------------------------
# Classes 
# ----------------------------------------------------------------------------


class NgramPreprocessor(Preprocessor):

	def __init__(self, features_count, benign_files_input_dir, malicious_files_input_dir, benign_files_output_dir, malware_files_output_dir, most_common_ngrams_file=''):
		super().__init__(features_count, benign_files_input_dir, malicious_files_input_dir)

		self._malware_files_output_dir = malware_files_output_dir
		self._benign_files_output_dir = benign_files_output_dir

		# if most common ngrams of this directory has already been processed- load that
		# otherwise - run preprocessing again
		if most_common_ngrams_file:
			ngram_matrix = np.genfromtxt(most_common_ngrams_file ,skip_header=True, dtype=str, delimiter=',')
			sorted_ngram_mat = sorted(list(ngram_matrix), reverse=True, key=lambda seq : int(seq[NGRAM_N]))
			most_common_ngram_features_mat = np.array(sorted_ngram_mat)[:self._features_count]
			self._most_common_sequences = most_common_ngram_features_mat[:,0:NGRAM_N]
			
		else:
			self._most_common_sequences = self._preprocess_input_files()


	def export_ngram_matrix(self, out_file_path, sequences_mat, sequence_freqs):
		# concatanete frequencies array to be the last column of the sequenes matrix
		sequence_freqs = sequence_freqs.reshape(len(sequence_freqs), 1)
		result_matrix = np.concatenate((sequences_mat, sequence_freqs), axis=1)

		if out_file_path:
			CSV_FILE_HEADER = 'OPCODE1, OPCODE2, FREQUENCY'
			np.savetxt(out_file_path, result_matrix , fmt='%s', delimiter=',', header=CSV_FILE_HEADER)

		return result_matrix


	def calc_ngram_pure_python(self,twogram_array):
		hist = [((hex(seq[0]),hex(seq[1])), twogram.count(seq)) for seq in unique_sequences[:HISTOGRAM_SIZE]]
		return hist

	def calc_ngram_numpy(self,twogram_array):
		unique_sequences, sequence_freqs = np.unique(twogram_array, axis=0, return_counts=True)
		return (unique_sequences, sequence_freqs)

	def extract_opcodes_list(self, file_path, is_pe):

		if is_pe:
			pe = pefile.PE(file_path)

			# Find text (code) section and extract it
			for section in pe.sections:
				if 'text' in str(section.Name):
					code_section = section
					break

			raw_code = code_section.get_data()
			code_section_size = code_section.SizeOfRawData

			# Determine pe file architecture
			if pe.FILE_HEADER.Machine == 0x014c:
				pe_architecutre = capstone.CS_MODE_32

			if pe.FILE_HEADER.Machine == 0x8664:
				pe_architecutre = capstone.CS_MODE_64

		else:
			input_file = open(file_path, 'rb')
			pe_architecutre = capstone.CS_MODE_32
			raw_code = input_file.read()

			raw_code = np.trim_zeros(raw_code[50:])
			code_section_size = len(raw_code)

			input_file.close()


		# initialize disassembler to of the correct architecture 
		disassembler = capstone.Cs(capstone.CS_ARCH_X86, pe_architecutre)
		opcodes_list = []

		"""
		Go over all text section where binary code should be
		Disassemble every chunck and add the opcodes found to the opcodes list
		"""
		disassembler.skipdata = True
		disassembly_generator = disassembler.disasm_lite(raw_code, 0)

		for instruction in disassembly_generator:
			opcodes_list.append(instruction[OPCODE_OFFSET_IN_INSTRUCTION])

		#opcodes_data_ratio = len(opcodes_list) / len(raw_code)
		#print(f"----------> OpcodesCount/FileSize = : {opcodes_data_ratio}")

		return opcodes_list


	#@timeit
	def extract_ngram_of_file(self, in_file_path, out_file_path='', is_pe=True):

		try:
			opcodes_list = self.extract_opcodes_list(in_file_path, is_pe)
			#print(f"Opcodes Count: {len(opcodes_list)}")

		    # Now extrat twogram vector out of the binary file's opcodes list
			twogram_iter = ngrams(opcodes_list, NGRAM_N)
			twogram_array = np.array(list(twogram_iter))
			unique_sequences, sequence_freqs = self.calc_ngram_numpy(twogram_array)

			return self.export_ngram_matrix(out_file_path, unique_sequences, sequence_freqs)

		except Exception as ex:
			msg = f'==========> ERROR EXTRACTING NGRAM of: {in_file_path} \t|' + str(ex)
			logging.getLogger().error(msg)
			return None



	def _preprocess_input_files(self):

		self._process_all_files_in_dir(self._benign_files_input_dir, self._benign_files_output_dir)
		self._process_all_files_in_dir(self._malicious_files_input_dir, self._malware_files_output_dir)

		# Now extract most common sequences of NGRAM to both malware and bening files
		return self._extract_most_common_sequences(self._malware_files_output_dir, TEMP_OUTPUT_FILENAME_MOST_COMMON_NGRAMS)



	def _process_all_files_in_dir(self, in_directory_path, out_directory_path, extention="exe"):

		for filepath in glob.iglob(f'{in_directory_path}/*.{extention}'):
			basename = os.path.basename(filepath)
			
			print('----------------------------------------------------------------------------')
			print('Processing:   ' + filepath)
			self.extract_ngram_of_file(filepath, f'{out_directory_path}/{basename}-ngram.csv', extention=="exe")
			print('----------------------------------------------------------------------------')

			

	def _extract_most_common_sequences(self, in_directory_path, output_most_common_seqs_filepath):

		all_tfs_dict = {}

		for filepath in glob.iglob(f'{in_directory_path}/*.csv'):
			ngram_mat = np.genfromtxt(filepath ,skip_header=True, dtype=str, delimiter=',')
			term_frequecies = ngram_mat[:,NGRAM_N].astype(np.int32)
			opcodes_mat = ngram_mat[:,0:NGRAM_N]

			print(f'Processing: {filepath} ')

			for index, opcodes_sequence in enumerate(opcodes_mat):
				opcodes_sequence = tuple(opcodes_sequence.tolist())

				if not opcodes_sequence in all_tfs_dict:
					all_tfs_dict[opcodes_sequence] = term_frequecies[index]

				else:
					all_tfs_dict[opcodes_sequence] = all_tfs_dict[opcodes_sequence] + term_frequecies[index]


		# Now create numpy array from that dictionary and export it to csv file
		sequenes_list = list(all_tfs_dict.keys())

		sequences_mat = np.array(sequenes_list)
		sequence_freqs = np.array(list(all_tfs_dict.values()))

		sequence_freqs = sequence_freqs.reshape(len(sequence_freqs), 1)
		result_ngram_matrix = np.concatenate((sequences_mat, sequence_freqs), axis=1)

		# Save most common sequences in temporary file 
		if output_most_common_seqs_filepath:
			CSV_FILE_HEADER = 'OPCODE1, OPCODE2, COMBINED FREQUENCIES'
			np.savetxt(output_most_common_seqs_filepath, result_ngram_matrix , fmt='%s', delimiter=',', header=CSV_FILE_HEADER)


		#ngram_mat = np.genfromtxt(most_common_ngrams_file ,skip_header=True, dtype=str, delimiter=',')
		sorted_ngram_mat = sorted(list(result_ngram_matrix), reverse=True, key=lambda seq : int(seq[NGRAM_N]))
		most_common_ngram_features_mat = np.array(sorted_ngram_mat)[:self._features_count]
		most_common_sequences_arr = most_common_ngram_features_mat[:,0:NGRAM_N]

		return most_common_sequences_arr


	def _create_partial_dataset(self, ngram_files_input_dir, temp_output_filepath =''):
		
		# For file in ngram_files_input_dir - create line in dataset file
		# For each sequence in most_common_ngram_features
		# if sequence not exsist in file_ngram ---> put 0 in vector cell
		# otherwise - put the sequen frequency which in the file (count/features count)

		dataset_list = []

		for filepath in glob.iglob(f'{ngram_files_input_dir}/*.csv'):
			print('Processing: ' + filepath)

			file_ngram_mat = np.genfromtxt(filepath ,skip_header=True, dtype=str, delimiter=',')
			ngram_total_features_count = sum(file_ngram_mat[:,NGRAM_N].astype(np.int32))

			# Build a dictionary of each of the file's sequences and it's term-frequency 
			file_ngrams_dict = {tuple(sequence[:NGRAM_N]) : float(sequence[NGRAM_N]) for sequence in file_ngram_mat}

			# Finally build a feature vector of the spesific file in the dataset 
			file_feature_vector = [file_ngrams_dict[tuple(sequence)]/ngram_total_features_count if tuple(sequence) in file_ngrams_dict else 0.0 for sequence in self._most_common_sequences]
			dataset_list.append(file_feature_vector)

		if temp_output_filepath:
			np.savetxt(temp_output_filepath, dataset , fmt='%s', delimiter=',')

		return np.array(dataset_list)


	def get_feature_vector(self, file_path):

		if not os.path.isfile(file_path):
			raise FeaturesExtractionException("File not exsists!")

		file_ngram_mat = self.extract_ngram_of_file(file_path)

		# If error occured while trying to get file's ngram matrix
		if file_ngram_mat is None:
			return None

		ngram_total_features_count = sum(file_ngram_mat[:,NGRAM_N].astype(np.int32))

		# Build a dictionary of each of the file's sequences and it's term-frequency 
		file_ngrams_dict = {tuple(sequence[:NGRAM_N]) : float(sequence[NGRAM_N]) for sequence in file_ngram_mat}

		# Finally build a feature vector of the spesific file in the dataset 
		file_feature_vector = [file_ngrams_dict[tuple(sequence)]/ngram_total_features_count if tuple(sequence) in file_ngrams_dict else 0.0 for sequence in self._most_common_sequences]

		return file_feature_vector



	def create_dataset(self, output_dataset_file):

		benign_files_features_matrix = self._create_partial_dataset(self._benign_files_output_dir)
		malicious_files_features_matrix = self._create_partial_dataset(self._malware_files_output_dir)

		# push to each matrix 1 coulumn of label (benign or malicious)
		benign_dataset = np.column_stack((benign_files_features_matrix, np.ones(benign_files_features_matrix.shape[0]) * LABEL_BENIGN))
		malicous_dataset = np.column_stack((malicious_files_features_matrix, np.ones(malicious_files_features_matrix.shape[0]) * LABEL_MALICIOUS))

		final_dataset = np.concatenate((benign_dataset,malicous_dataset))
		np.savetxt(output_dataset_file, final_dataset , fmt='%s', delimiter=',')

